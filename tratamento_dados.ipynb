{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa funções para o pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_processamento import *\n",
    "\n",
    "from dlisio import dlis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# A função 'glob' do módulo 'glob' é usada para procurar todos os arquivos em um diretório com determinada extensão\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carrega os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes_arquivos = []     # Armazena os nomes dos arquivos .dlis\n",
    "leituras_dlis = []      # Armazena as leituras dos arquivos .dlis\n",
    "nomes_anp = []          # Armazena os nomes obtidos das leituras\n",
    "\n",
    "for file in glob.glob(r'**/Data' + \"/*.dlis\", recursive=True):\n",
    "    # Salva o nome do arquivo\n",
    "    nomes_arquivos.append(file)\n",
    "\n",
    "    # Salva os dados da leitura\n",
    "    leitura, *tail = dlis.load(f'{file}')\n",
    "    leituras_dlis.append(leitura)\n",
    "\n",
    "    # Salva o nome do poço\n",
    "    nome = leitura.origins[0].well_name\n",
    "    nomes_anp.append(nome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data\\\\1-brsa-551-se_brsa_raw.dlis', 'Data\\\\1-brsa-574-se_brsa_raw.dlis', 'Data\\\\1-brsa-595-se_brsa_raw-1.dlis', 'Data\\\\1-brsa-605-se_brsa_raw.dlis', 'Data\\\\1-brsa-659-se_brsa_raw.dlis', 'Data\\\\3-CP-1847-SE.dlis', 'Data\\\\3-CP-1848-SE.dlis', 'Data\\\\3-CP-1851-SE.dlis', 'Data\\\\3-CP-1853-SE.dlis', 'Data\\\\3-CP-1855-SE.dlis', 'Data\\\\3-CP-1857-SE.dlis']\n",
      "['1-FSG-1-SE', '1-FSJQ-1-SE', '1-BRSA-595-SE', '1-BRSA-605-SE', '1-BRSA-659-SE', '3-BRSA-900-SE', '3-BRSA-889-SE', '3-BRSA-912-SE', '3-BRSA-897-SE', '3-BRSA-910-SE', '3-BRSA-907-SE']\n"
     ]
    }
   ],
   "source": [
    "print(nomes_arquivos)\n",
    "print(nomes_anp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padroniza nomes fora do padão 'X-BRSA-XXX-SE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes_anp[0] = '1-BRSA-551-SE'\n",
    "nomes_anp[1] = '1-BRSA-574-SE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-BRSA-551-SE',\n",
       " '1-BRSA-574-SE',\n",
       " '1-BRSA-595-SE',\n",
       " '1-BRSA-605-SE',\n",
       " '1-BRSA-659-SE',\n",
       " '3-BRSA-900-SE',\n",
       " '3-BRSA-889-SE',\n",
       " '3-BRSA-912-SE',\n",
       " '3-BRSA-897-SE',\n",
       " '3-BRSA-910-SE',\n",
       " '3-BRSA-907-SE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nomes_anp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria dicionário para armazenar os dados e respectivos nomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1-BRSA-551-SE': LogicalFile(AIT_SONIC_TLD_MCFL_018PUP),\n",
       " '1-BRSA-574-SE': LogicalFile(GEOLOAD.1),\n",
       " '1-BRSA-595-SE': LogicalFile(GEOLOAD.1),\n",
       " '1-BRSA-605-SE': LogicalFile(AIT_SONIC_TLD_MCFL_048PUC),\n",
       " '1-BRSA-659-SE': LogicalFile(GEOLOAD.1),\n",
       " '3-BRSA-900-SE': LogicalFile(GEOLOAD.1),\n",
       " '3-BRSA-889-SE': LogicalFile(GEOLOAD.1),\n",
       " '3-BRSA-912-SE': LogicalFile(GEOLOAD.1),\n",
       " '3-BRSA-897-SE': LogicalFile(GEOLOAD.1),\n",
       " '3-BRSA-910-SE': LogicalFile(GEOLOAD.1),\n",
       " '3-BRSA-907-SE': LogicalFile(GEOLOAD.1)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Casa itens da lista 'nome_anp_abreviados' com os itens da lista 'leituras_dlis'\n",
    "pares = zip(nomes_anp, leituras_dlis)\n",
    "\n",
    "# Cria dicionário 'dli_dict'\n",
    "dli_dict = dict(pares)\n",
    "dli_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separa TODAS as curvas presentes nos .dlis de cada poço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_dict = {}\n",
    "\n",
    "for key, poco in dli_dict.items():\n",
    "    channels_list = []\n",
    "    for frame in poco.frames:\n",
    "        channels = frame.channels\n",
    "        channels_list.append([channel.name for channel in channels])\n",
    "    channels_dict[key] = sum(channels_list, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatena os arquivos CSV das curvas em um único arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo curvas_pocos.csv criado com sucesso\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pasta = \"Curvas_CSV\"\n",
    "\n",
    "# Lista para armazenar os dataframes repectivos aos .csv\n",
    "df_list = []\n",
    "\n",
    "# Itera sobre a pasta Curvas_CSV\n",
    "for arquivo in os.listdir(pasta):\n",
    "    file_path = os.path.join(pasta, arquivo)\n",
    "\n",
    "    # Lê o arquivo CSV como um DF\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatena os DFs ao longo do eixo das colunas\n",
    "df_concat = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "# Salva o DF concatenado em um .csv\n",
    "df_concat.to_csv(\"curvas_pocos.csv\", index=False)\n",
    "\n",
    "print(\"Arquivo curvas_pocos.csv criado com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria dataframes para os poços"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns poços possuem mais de um 'frame'.\n",
    "\n",
    "Irei criar um 'dataframe' para cada um desses 'frames'.\n",
    "\n",
    "Irei fazer um 'JOIN' usando a coluna 'TDEP'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cria um dataframe para cada frame do poço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando o teste com um poço apenas\n",
    "poco = dli_dict['1-BRSA-605-SE']\n",
    "\n",
    "# Dicionário para armazenar os dataframes dos frames do poço 605\n",
    "dataframes_605 = {}\n",
    "\n",
    "# Iterando sobre os frames do poço\n",
    "for frame in poco.frames:\n",
    "    # indice do frame\n",
    "    indice = poco.frames.index(frame)\n",
    "    \n",
    "    # obtendo os nomes das curvas \n",
    "    channels = frame.channels\n",
    "    curvas_nomes = [channel.name for channel in channels]\n",
    " \n",
    "    # obtendo os valores das curvas\n",
    "    curvas_valores = frame.curves()\n",
    "\n",
    "    dataframes_605[indice] = pd.DataFrame(curvas_valores, columns=curvas_nomes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(dataframes_605[0], dataframes_605[1], on='TDEP', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TDEP</th>\n",
       "      <th>BS</th>\n",
       "      <th>TENS</th>\n",
       "      <th>SP</th>\n",
       "      <th>GR</th>\n",
       "      <th>NPHI</th>\n",
       "      <th>TIME_x</th>\n",
       "      <th>IHV</th>\n",
       "      <th>ICV</th>\n",
       "      <th>DT</th>\n",
       "      <th>TIME_y</th>\n",
       "      <th>HCAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264240.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>135.3750</td>\n",
       "      <td>47.26149</td>\n",
       "      <td>0.263958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.793983</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.887136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>264180.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>136.3125</td>\n",
       "      <td>47.26149</td>\n",
       "      <td>0.263958</td>\n",
       "      <td>2184.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.793983</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.887136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>264120.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1811.0</td>\n",
       "      <td>135.7500</td>\n",
       "      <td>47.26149</td>\n",
       "      <td>0.263958</td>\n",
       "      <td>1879.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.793983</td>\n",
       "      <td>313.16748</td>\n",
       "      <td>3.887136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264060.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1776.0</td>\n",
       "      <td>136.2500</td>\n",
       "      <td>47.26149</td>\n",
       "      <td>0.263958</td>\n",
       "      <td>1297.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.793983</td>\n",
       "      <td>216.25000</td>\n",
       "      <td>3.887136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>264000.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1871.0</td>\n",
       "      <td>135.3750</td>\n",
       "      <td>47.26149</td>\n",
       "      <td>0.263958</td>\n",
       "      <td>1131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.793983</td>\n",
       "      <td>188.50000</td>\n",
       "      <td>3.887136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25430</th>\n",
       "      <td>9950.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134.75000</td>\n",
       "      <td>3.812884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25431</th>\n",
       "      <td>9940.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134.75000</td>\n",
       "      <td>3.812884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25432</th>\n",
       "      <td>9930.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134.75000</td>\n",
       "      <td>3.823491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25433</th>\n",
       "      <td>9920.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134.75000</td>\n",
       "      <td>3.823491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25434</th>\n",
       "      <td>9910.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134.75000</td>\n",
       "      <td>3.823491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25435 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           TDEP   BS    TENS        SP        GR      NPHI  TIME_x  IHV  ICV  \\\n",
       "0      264240.0  8.5  2009.0  135.3750  47.26149  0.263958     0.0  0.0  0.0   \n",
       "1      264180.0  8.5  1731.0  136.3125  47.26149  0.263958  2184.0  0.0  0.0   \n",
       "2      264120.0  8.5  1811.0  135.7500  47.26149  0.263958  1879.0  0.0  0.0   \n",
       "3      264060.0  8.5  1776.0  136.2500  47.26149  0.263958  1297.5  0.0  0.0   \n",
       "4      264000.0  8.5  1871.0  135.3750  47.26149  0.263958  1131.0  0.0  0.0   \n",
       "...         ...  ...     ...       ...       ...       ...     ...  ...  ...   \n",
       "25430    9950.0  NaN     NaN       NaN       NaN       NaN     NaN  NaN  NaN   \n",
       "25431    9940.0  NaN     NaN       NaN       NaN       NaN     NaN  NaN  NaN   \n",
       "25432    9930.0  NaN     NaN       NaN       NaN       NaN     NaN  NaN  NaN   \n",
       "25433    9920.0  NaN     NaN       NaN       NaN       NaN     NaN  NaN  NaN   \n",
       "25434    9910.0  NaN     NaN       NaN       NaN       NaN     NaN  NaN  NaN   \n",
       "\n",
       "              DT     TIME_y      HCAL  \n",
       "0      82.793983    0.00000  3.887136  \n",
       "1      82.793983    0.00000  3.887136  \n",
       "2      82.793983  313.16748  3.887136  \n",
       "3      82.793983  216.25000  3.887136  \n",
       "4      82.793983  188.50000  3.887136  \n",
       "...          ...        ...       ...  \n",
       "25430        NaN  134.75000  3.812884  \n",
       "25431        NaN  134.75000  3.812884  \n",
       "25432        NaN  134.75000  3.823491  \n",
       "25433        NaN  134.75000  3.823491  \n",
       "25434        NaN  134.75000  3.823491  \n",
       "\n",
       "[25435 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Passing 'suffixes' which cause duplicate columns {'TIME_x'} is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m df_concatenado \u001b[38;5;241m=\u001b[39m dataframes_605_listas[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m dataframes_605_listas[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m----> 6\u001b[0m     df_concatenado \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_concatenado\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTDEP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rafae\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:183\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    170\u001b[0m         left_df,\n\u001b[0;32m    171\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rafae\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:885\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m    883\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 885\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    888\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[1;32mc:\\Users\\rafae\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:837\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    834\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft[:]\n\u001b[0;32m    835\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright[:]\n\u001b[1;32m--> 837\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m \u001b[43m_items_overlap_with_suffix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffixes\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[0;32m    845\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    846\u001b[0m         join_index,\n\u001b[0;32m    847\u001b[0m         left_indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    852\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    853\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rafae\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:2691\u001b[0m, in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, right, suffixes)\u001b[0m\n\u001b[0;32m   2689\u001b[0m     dups\u001b[38;5;241m.\u001b[39mextend(rlabels[(rlabels\u001b[38;5;241m.\u001b[39mduplicated()) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m~\u001b[39mright\u001b[38;5;241m.\u001b[39mduplicated())]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m   2690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dups:\n\u001b[1;32m-> 2691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[0;32m   2692\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuffixes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m which cause duplicate columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(dups)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2693\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2694\u001b[0m     )\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m llabels, rlabels\n",
      "\u001b[1;31mMergeError\u001b[0m: Passing 'suffixes' which cause duplicate columns {'TIME_x'} is not allowed."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "dataframes_605_listas = list(dataframes_605.values())\n",
    "\n",
    "df_concatenado = dataframes_605_listas[0]\n",
    "\n",
    "for df in dataframes_605_listas[1:]:\n",
    "    df_concatenado = pd.merge(df_concatenado, df, on='TDEP', how='outer')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'GR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m poco\u001b[38;5;241m.\u001b[39mframes:\n\u001b[0;32m     25\u001b[0m     curvas \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mcurves()\n\u001b[1;32m---> 27\u001b[0m     dataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([dataframe, pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mcurvas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurvas_utilizadas_sem_duplicados\u001b[49m\u001b[43m]\u001b[49m)])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Criando um pandas dataframe do poço respectivo à atual iteração e armazenando o mesmo em dlis_df\u001b[39;00m\n\u001b[0;32m     30\u001b[0m dlis_df_dict[chave] \u001b[38;5;241m=\u001b[39m dataframe\n",
      "\u001b[1;31mKeyError\u001b[0m: 'GR'"
     ]
    }
   ],
   "source": [
    "dlis_df_dict = {}   # Conterá os dataframes respectivos aos poços\n",
    "\n",
    "# Curvas de perfis escolhidas\n",
    "curvas_escolhidas = ['TDEP', 'GR', 'NPHI', 'RHOB', 'DRHO', 'HDRS', 'LLD', 'BSZ', 'BS', 'CALI', 'DCALI', 'DCAL', 'PE', 'DTC']\n",
    "\n",
    "# Iterando sobre os arquivos lógicos de todos os poços (que estão armazenados em 'dli_dict'),\n",
    "# '.values()' se refere aos valores do dicionário (não às chaves)\n",
    "for chave, poco in dli_dict.items():\n",
    "\n",
    "    # Armazenando as curvas que serão utilizadas em uma lista\n",
    "    curvas_utilizadas = [\n",
    "        channel.name                                    # Os elementos da lista serão os nomes das curvas\n",
    "        for channel in poco.channels                    # As curvas são acessadas por meio de 'poco.channels'\n",
    "        if channel.name in curvas_escolhidas            # As curvas que não utilizaremos não serão armazenadas na lista\n",
    "    ]\n",
    "    conjunto_aux = set(curvas_utilizadas)\n",
    "    curvas_utilizadas_sem_duplicados = list(conjunto_aux)\n",
    "    \n",
    "    #frames = [np.atleast_1d(frame) for frame in poco.frames]\n",
    "    #curvas = np.concatenate(frames, axis=0)\n",
    "    dataframe = pd.DataFrame()\n",
    "    \n",
    "    for frame in poco.frames:\n",
    "\n",
    "        curvas = frame.curves()\n",
    "\n",
    "        dataframe = pd.concat([dataframe, pd.DataFrame(curvas[curvas_utilizadas_sem_duplicados])])\n",
    "\n",
    "    # Criando um pandas dataframe do poço respectivo à atual iteração e armazenando o mesmo em dlis_df\n",
    "    dlis_df_dict[chave] = dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['551', '574', '595', '605', '659', '900', '889', '912', '897', '910', '907'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlis_df_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforma os valores -999.25 em nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for poco in dlis_df_dict.values():\n",
    "    poco.replace([-999.25], [None], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando os mnemônicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aplica_mnemonico(dlis_df_dict, ['BS', 'BSZ'], 'BS')\n",
    "aplica_mnemonico(dlis_df_dict, ['LLD', 'HDRS'], 'RESD')\n",
    "aplica_mnemonico(dlis_df_dict, ['RHOB', 'RHOZ'], 'RHOB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renomeia CALI para CAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for poco in dlis_df_dict.values():\n",
    "    renomeia_coluna(poco, 'CALI', 'CAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adiciona coluna DCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_DCAL(dlis_df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preenche os poços com curvas faltando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se um dos poços não tiver uma dessas curvas, adicionamos a coluna da curva e mantemos os valores como None\n",
    "curvas_obrigatorias = ['BS', 'CAL', 'DCAL', 'DRHO', 'DTC', 'GR', 'NPHI', 'PE', 'RESD', 'RHOB', 'TDEP']\n",
    "\n",
    "# Percorre todos os poços\n",
    "for poco in dlis_df_dict.values():\n",
    "    # Percorre todas as curvas obrigatórias\n",
    "    for curva in curvas_obrigatorias:\n",
    "        # Se o poço não tiver a curva\n",
    "        if curva not in poco.columns:\n",
    "            # Adiciona a coluna e os valores dela = None\n",
    "            poco[curva] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temos as seguintes curvas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551: ['BS', 'CAL', 'DCAL', 'DRHO', 'DTC', 'GR', 'NPHI', 'PE', 'RESD', 'RHOB', 'TDEP']\n",
      "574: ['BS', 'CAL', 'DCAL', 'DRHO', 'DTC', 'GR', 'NPHI', 'PE', 'RESD', 'RHOB', 'TDEP']\n",
      "595: ['BS', 'CAL', 'DCAL', 'DRHO', 'DTC', 'GR', 'NPHI', 'PE', 'RESD', 'RHOB', 'TDEP']\n",
      "605: ['BS', 'CAL', 'DCAL', 'DRHO', 'DTC', 'GR', 'NPHI', 'PE', 'RESD', 'RHOB', 'TDEP']\n",
      "659: ['BS', 'CAL', 'DCAL', 'DRHO', 'DTC', 'GR', 'NPHI', 'PE', 'RESD', 'RHOB', 'TDEP']\n",
      "900: ['BS', 'CAL', 'DCAL', 'DRHO', 'DTC', 'GR', 'NPHI', 'PE', 'RESD', 'RHOB', 'TDEP']\n",
      "889: ['BS', 'CAL', 'DCAL', 'DRHO', 'DTC', 'GR', 'NPHI', 'PE', 'RESD', 'RHOB', 'TDEP']\n",
      "912: ['BS', 'CAL', 'DCAL', 'DRHO', 'DTC', 'GR', 'NPHI', 'PE', 'RESD', 'RHOB', 'TDEP']\n",
      "897: ['BS', 'CAL', 'DCAL', 'DRHO', 'DTC', 'GR', 'NPHI', 'PE', 'RESD', 'RHOB', 'TDEP']\n",
      "910: ['BS', 'CAL', 'DCAL', 'DRHO', 'DTC', 'GR', 'NPHI', 'PE', 'RESD', 'RHOB', 'TDEP']\n",
      "907: ['BS', 'CAL', 'DCAL', 'DRHO', 'DTC', 'GR', 'NPHI', 'PE', 'RESD', 'RHOB', 'TDEP']\n"
     ]
    }
   ],
   "source": [
    "for key, poco in dlis_df_dict.items():\n",
    "    curvas = sorted(poco.keys())\n",
    "    print(f\"{key}: {curvas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove valores DRHO e DCAL indesejados (Só depois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limita_curva(dlis_df_dict, \"DRHO\", -0.15, 0.15)\n",
    "#limita_curva(dlis_df_dict, \"DCAL\", -2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverte a ordem das linhas dos dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dlis_df_dict.keys():\n",
    "    dlis_df_dict[key] = dlis_df_dict[key].iloc[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordena os dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordem_desejada = ['TDEP', 'GR', 'RESD', 'BS', 'CAL', 'DCAL', 'NPHI', 'PE', 'DRHO', 'RHOB', 'DTC']\n",
    "\n",
    "for key in dlis_df_dict.keys():\n",
    "    dlis_df_dict[key] = dlis_df_dict[key].reindex(columns=ordem_desejada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva os dados dos dataframes em arquivos CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo Pocos_CSV/poco_551.csv criado com sucesso.\n",
      "Arquivo Pocos_CSV/poco_574.csv criado com sucesso.\n",
      "Arquivo Pocos_CSV/poco_595.csv criado com sucesso.\n",
      "Arquivo Pocos_CSV/poco_605.csv criado com sucesso.\n",
      "Arquivo Pocos_CSV/poco_659.csv criado com sucesso.\n",
      "Arquivo Pocos_CSV/poco_900.csv criado com sucesso.\n",
      "Arquivo Pocos_CSV/poco_889.csv criado com sucesso.\n",
      "Arquivo Pocos_CSV/poco_912.csv criado com sucesso.\n",
      "Arquivo Pocos_CSV/poco_897.csv criado com sucesso.\n",
      "Arquivo Pocos_CSV/poco_910.csv criado com sucesso.\n",
      "Arquivo Pocos_CSV/poco_907.csv criado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "for key, value in dlis_df_dict.items():\n",
    "    file_name = f\"Pocos_CSV/poco_{key}.csv\"\n",
    "    value.to_csv(file_name, index=False)\n",
    "    print(f\"Arquivo {file_name} criado com sucesso.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
